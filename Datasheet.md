#  Datasheet: Black Box Optimization (BBO) Capstone Challenge Data
1. Motivation
•	For what purpose was the dataset created?
The dataset was created to track, analyse, and solve a Sequential Model-Based Optimization (SMBO) challenge. The primary objective is to maximize (or minimize) the output of eight distinct "black box" functions where the internal logic is unknown.
•	Who created the dataset?
This dataset is a cumulative record generated by the researcher during the 13-week Capstone BBO Challenge.
•	What tasks could the dataset be used for?
o	Bayesian Optimization: Testing acquisition functions (UCB, EI, PI) and surrogate models (Gaussian Processes).
o	Regression Analysis: Modeling high-dimensional landscapes with sparse data.
o	Hyperparameter Tuning: Benchmarking strategies for finding global optima in non-convex, noisy, or high-dimensional spaces.
2. Composition
•	What do the instances that comprise the dataset represent?
Each instance represents a single query to a black box function. It consists of a specific configuration of input parameters (hyperparameters) and the resulting scalar performance metric (output).
•	How many instances are there in total?
The dataset is partitioned into 8 subsets (Functions 1–8).
o	Initial Data: Varies per function (typically 10–40 points).
o	Cumulative Data: Increases weekly. By Week 10, each function contains approximately 20–50 total data points.
•	What is the format of the data?
o	Inputs ($X$): Continuous floating-point values, generally bounded within the hypercube $[0, 1]^d$, where $d$ ranges from 2 (Function 1) to 8 (Function 8).
o	Outputs ($y$): Continuous floating-point scalar values.
•	Are there any known gaps or anomalies?
o	Sparsity: High-dimensional functions (F7, F8) are extremely sparse due to the "curse of dimensionality" and limited query budget.
o	Noise: Function 2 contains stochastic noise; identical inputs may yield different outputs.
o	Silent Regions: Function 1 contains a large "plateau" where outputs are effectively zero ($\approx 10^{-16}$), representing a "needle-in-a-haystack" anomaly.
3. Collection Process
•	How was the data associated with each instance acquired?
o	Initial Data: Provided by the challenge organizers, likely generated via Random Sampling or Latin Hypercube Sampling (LHS) to ensure initial coverage.
o	Weekly Data: Generated iteratively by the researcher using Python-based optimization libraries (scikit-learn, scipy).
•	What strategy was used to generate the queries?
A hybrid strategy evolving over time:
o	Weeks 1–5 (Exploration): High-variance strategies (e.g., Upper Confidence Bound with high kappa) to map the global landscape.
o	Weeks 6–9 (Transition): Shift toward Expected Improvement (EI).
o	Weeks 10–13 (Exploitation): Trust Region optimization and local fine-tuning around the best-known basins of attraction.
•	Over what timeframe was the data collected?
The data collection occurs sequentially over a 13-week period, with one batch of queries submitted and evaluated per week.
4. Preprocessing and Uses
•	Was any preprocessing/cleaning done?
o	Inputs: No transformation applied (raw inputs used).
o	Outputs: Standardization (StandardScaler) is applied dynamically during the modeling phase to ensure outputs have zero mean and unit variance, facilitating Gaussian Process convergence.
•	What are the intended uses?
o	Optimizing the specific 8 functions provided in the challenge.
o	Analyzing the "fingerprints" of unknown functions (e.g., assessing smoothness, linearity, or noise levels).
•	What are the inappropriate uses?
o	Deep Learning: The dataset size is too small (<100 points) to train neural networks effectively.
o	Extrapolation: Predictions made outside the bounds $[0, 1]$ are unreliable as the surrogate models are only valid within the search space.
5. Distribution and Maintenance
•	Where is the dataset available?
The data is stored locally in Jupyter Notebooks (.ipynb) and serialized files (.npy, .csv) within the researcher's project directory.
•	Who is supporting/hosting/maintaining the dataset?
The dataset is maintained by the primary researcher (User) for the duration of the Capstone project.
•	What are the terms of use?
Restricted to the context of the Capstone challenge. Future use is subject to the academic integrity guidelines of the host institution.

